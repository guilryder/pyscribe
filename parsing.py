# -*- coding: utf-8 -*-
# Copyright 2011, Guillaume Ryder, GNU GPL v3 license

__author__ = 'Guillaume Ryder'

import collections
import enum
import inspect
import itertools
import re

from log import Filename, Location
from macros import MACRO_NAME_PATTERN, VALID_MACRO_NAME_REGEXP


class TokenType(enum.Enum):
  LBRACKET = enum.auto()  # value: ignored
  RBRACKET = enum.auto()  # value: ignored
  TEXT = enum.auto()      # value: unescaped text
  MACRO = enum.auto()     # value: macro name without '$' prefix


class Token:
  """
  Token generated by Lexer.

  Fields:
    type: (int) The type of the token, must be one of TOKEN_* constants.
    lineno: (int) The line index of the first character of the token.
    value: (String) The contents of the token.
  """

  def __init__(self, tok_type, lineno, value):
    self.type = tok_type
    self.lineno = lineno
    self.value = value

  def __repr__(self):
    return f'({self.type} l{self.lineno} {self.value!r})'


class TextNode:
  """Plain text node; may contain line breaks."""

  def __init__(self, location, text):
    self.location = location
    self.text = text

  def Execute(self, executor):
    executor.AppendText(self.text)

  def __str__(self):
    return repr(self.text)

  def __repr__(self):
    return f'{self.location!r}{self}'

  def __eq__(self, other):
    return isinstance(other, TextNode) and repr(self) == repr(other)


class CallNode:
  """
  Macro call node.

  Fields:
    location: (Location) The location of the call node.
    name: (str) The name of the macro called, without '$' prefix.
    args: (List[List[node]]) The arguments of the macro, each as a node list.
  """

  def __init__(self, location, name, args):
    self.location = location
    self.name = name
    self.args = args

  def Execute(self, executor):
    executor.CallMacro(self)

  def __str__(self):
    args = ''.join(f'[{FormatNodes(param)}]' for param in self.args)
    return f'${self.name}{args}'

  def __repr__(self):
    args = ''.join(f'[{ReprNodes(param)}]' for param in self.args)
    return f'${self.name}{args}'

  def __eq__(self, other):
    return (isinstance(other, CallNode) and
            self.location == other.location and
            self.name == other.name and
            self.args == other.args)


def CompactTextNodes(nodes):
  """
  Merges consecutive text nodes located on the same line.

  Args:
    nodes: (Node iter) The nodes to process.

  Yields:
    (Node) The input nodes with consecutive text nodes merged. The location of a
    merged text node is the location of the first original text node.
  """
  def GroupingKey(node):
    return (type(node), node.location)
  for (node_type, location), nodes_grp in itertools.groupby(nodes, GroupingKey):
    if issubclass(node_type, TextNode):
      yield TextNode(location, ''.join(node.text for node in list(nodes_grp)))
    else:
      yield from nodes_grp


def FormatNodes(nodes):
  """Formats the given text nodes into a human-readable string."""
  return ''.join(str(node) for node in CompactTextNodes(nodes))


def ReprNodes(nodes):
  """Formats the given text nodes into a representation string."""
  return ''.join(repr(node) for node in nodes)


class ParsingContext:
  """
  Context for parsing an input file.

  Fields:
    filename: (Filename) The name of the file parsed.
    logger: (Logger) The logger to use to report errors
  """

  def __init__(self, filename, logger):
    assert isinstance(filename, Filename)
    self.filename = filename
    self.logger = logger

  def MakeLocation(self, lineno):
    """Builds a Location object for this context."""
    return Location(self.filename, lineno)

  def MakeLocationError(self, *args, **kwargs):
    """
    Returns a fatal error for the current location.

    See Logger.LocationError() for parameters.
    """
    return self.logger.LocationError(*args, **kwargs)


_RULE_METHOD_NAME_PATTERN = re.compile('Rule(?P<name>.+)')

class RuleType(enum.Flag):
  REGULAR = enum.auto()
  SPECIAL_CHAR_LATEX = enum.auto()
  SPECIAL_CHAR_OTHER = enum.auto()
  ALL = REGULAR | SPECIAL_CHAR_LATEX | SPECIAL_CHAR_OTHER

def rule(regexp, rule_type=RuleType.REGULAR):
  """Decorator for rule methods; see RegexpParser."""
  def wrapper(func):
    name_match = _RULE_METHOD_NAME_PATTERN.match(func.__name__)
    assert name_match is not None, 'Invalid @rule function name: ' + func.name
    func.rule_name = name_match.group('name')
    func.regexp = regexp
    func.rule_type = rule_type
    return func
  return wrapper


class RegexpParser:
  """
  Regexp-based parser.

  Creates a parsing rule for each method of an object decorated with @rule.

  Fields:
    __rules: (OrderedDict[name, method])
  """

  def __init__(self, rules_container):
    """
    Args:
      rules_container: (object) The object that contains the @rule methods.
    """

    # Retrieve the rules from the container, sorted alphabetically.
    self.__rules = collections.OrderedDict((
        (rule.rule_name, rule)
        for _, rule in inspect.getmembers(rules_container,
                                          lambda mem: hasattr(mem, 'rule_name'))
    ))

    # Compute the aggregate regexp for all rules.
    full_pattern = '|'.join(
        f'(?P<{rule.rule_name}>{rule.regexp})'
        for rule in self.__rules.values())
    self.__regexp = re.compile(full_pattern, re.MULTILINE)

  def Parse(self, input_text):
    """
    Parses the given input text.

    Args:
      input_text: (str) The text to parse.

    Yields:
      (Tuple[str, callable|None, str|None]) The non-overlapping matches of the
      rules, each as a Tuple[text_before, rule_callable, matched_text].
      'text_before' is the text between the previous match (or the beginning of
      the string for the first match) and the current match.
      'rule_callable' and 'matched_text' are the rule and the text that matched,
      None after the last match if there is text after it.
    """
    rules = self.__rules
    last_end = 0

    for match in self.__regexp.finditer(input_text):
      (match_begin, match_end) = match.span()
      rule_name = match.lastgroup
      yield (input_text[last_end:match_begin],
             rules[rule_name], match.group(rule_name))
      last_end = match_end

    if last_end < len(input_text):
      yield (input_text[last_end:], None, None)


class Lexer:

  __GLOBAL_STRIP_REGEXP = re.compile(r'(?:|\^|\s*(.*?)(?<=[^\^])\s*\^?)\Z',
                              re.MULTILINE | re.DOTALL)
  __LINE_REGEXP = re.compile(r'[ \t]*(\n+)[ \t]*')

  def __init__(self, context, input_text):
    # Strip whitespace around the input text.
    input_text_match = self.__GLOBAL_STRIP_REGEXP.match(input_text)
    self.__lineno = 1 + input_text[:input_text_match.start(1)].count('\n')
    self.__skip_spaces = True
    self.__enabled_rule_types = RuleType.ALL
    self.__input_text = input_text_match.group(1) or ''

    self.context = context
    self.__filename = context.filename
    self.__parser = RegexpParser(self)
    self.__tokens = self.__MergeTextTokensSameLine(self.__Parse())

    def SetEnabledRuleTypes(bitset):
      self.__enabled_rule_types = bitset

    self.__preproc_instr_callbacks = {
        'whitespace.preserve': self.__PreprocessWhitespacePreserve,
        'whitespace.skip': self.__PreprocessWhitespaceSkip,
        'special.chars.escape.all': lambda: SetEnabledRuleTypes(
            RuleType.ALL),
        'special.chars.escape.none': lambda: SetEnabledRuleTypes(
            RuleType.REGULAR),
        'special.chars.latex.mode': lambda: SetEnabledRuleTypes(
            RuleType.REGULAR | RuleType.SPECIAL_CHAR_OTHER),
    }
    self.__text_processor = self.__TextProcessorPreserveWhitespace

  def __iter__(self):
    """Returns the tokens iterator."""
    return self.__tokens

  @staticmethod
  def __MergeTextTokensSameLine(tokens):
    """
    Merges the consecutive text tokens that have the same line.

    Args:
      nodes: (iter[Token]) The tokens to process.

    Yields:
      (Token)
    """
    text_token_accu = None
    for token in tokens:
      if token.type == TokenType.TEXT:
        # Text token
        if text_token_accu is None:
          # Start a new text accumulator.
          text_token_accu = token
        else:
          if text_token_accu.lineno == token.lineno:
            # Same line: append the text token to the accumulator.
            text_token_accu.value += token.value
            continue
          yield text_token_accu
          text_token_accu = None
          yield token
      else:
        # Not a text token: flush the text accumulator (if any) then the token.
        if text_token_accu is not None:
          yield text_token_accu
          text_token_accu = None
        yield token

    if text_token_accu is not None:
      yield text_token_accu

  def __Parse(self):
    """
    Parses the tokens.

    Yields:
      (Token) The parsed tokens.
    """
    # pylint: disable=import-outside-toplevel
    from collections.abc import Iterable
    for text_before, rule_callable, matched_text in (
        self.__parser.Parse(self.__input_text)):
      if text_before is not None:
        if self.__skip_spaces:
          text_before = text_before.lstrip(' \t')
        yield from self.__text_processor(text_before)
        self.__skip_spaces = (text_before and text_before[-1] == '\n')
      if rule_callable is not None:
        if self.__enabled_rule_types & rule_callable.rule_type:
          token = rule_callable(matched_text)
          if isinstance(token, Iterable):
            yield from token
          elif token is not None:
            yield token
        else:
          yield Token(TokenType.TEXT, self.__lineno, matched_text)

  def __TextProcessorPreserveWhitespace(self, text):
    """
    Yields tokens for a block of text, preserving whitespace.

    Ensures that line returns are always at the end of a token.
    Strips spaces around each line.
    """
    lineno = self.__lineno

    # Yield the text before each sequence of newlines,
    # including the newlines themselves but without surrounding spaces.
    last_end = 0
    for match in self.__LINE_REGEXP.finditer(text):
      (match_begin, match_end) = match.span()
      newlines = match.group(1)
      yield Token(TokenType.TEXT, lineno, text[last_end:match_begin] + newlines)
      lineno += len(newlines)
      last_end = match_end

    # Yield the last chunk of text.
    if last_end < len(text):
      yield Token(TokenType.TEXT, lineno, text[last_end:])

    self.__lineno = lineno
    self.__skip_spaces = False
  __TextProcessorPreserveWhitespace.skip_whitespace_after_macro = False

  def __TextProcessorSkipWhitespace(self, text):
    """
    Sames as __TextProcessorPreserveWhitespace, but skips more whitespace.

    Skips newlines and whitespace after macros.
    """
    lineno = self.__lineno

    # Yield the text before each sequence of newlines.
    last_end = 0
    for match in self.__LINE_REGEXP.finditer(text):
      (match_begin, match_end) = match.span()
      if last_end < match_begin:
        yield Token(TokenType.TEXT, lineno, text[last_end:match_begin])
      lineno += len(match.group(1))
      last_end = match_end

    # Yield the last chunk of text.
    if last_end < len(text):
      yield Token(TokenType.TEXT, lineno, text[last_end:])

    self.__lineno = lineno
    self.__skip_spaces = False
  __TextProcessorSkipWhitespace.skip_whitespace_after_macro = True

  def __Location(self):
    return Location(self.__filename, self.__lineno)

  def __UpdateLineno(self, text):
    if text:
      self.__lineno += text.count('\n')
      self.__skip_spaces = (text[-1] == '\n')

  @rule(r'[ \t]*\#.*(?:\n\s*|\Z)')
  def RuleComment(self, value):
    self.__UpdateLineno(value)

  @rule(r'\^.')
  def RuleEscape(self, value):
    self.__skip_spaces = False
    return Token(TokenType.TEXT, self.__lineno, value[1:])

  @rule(r'\[\s*')
  def RuleLbracket(self, value):
    token = Token(TokenType.LBRACKET, self.__lineno, '[')
    self.__UpdateLineno(value)
    return token

  @rule(r'\s*\]')
  def RuleRbracket(self, value):
    self.__UpdateLineno(value)
    return Token(TokenType.RBRACKET, self.__lineno, value)

  # Pre-processing statement

  @rule(r'\$\$[a-zA-Z0-9._]*\n?')
  def RulePreProcessingInstruction(self, value):
    preproc_instr_name = value[2:].strip()
    preproc_instr_callback = (
        self.__preproc_instr_callbacks.get(preproc_instr_name))
    if preproc_instr_callback is None:
      known = ', '.join(sorted(
          f'$${name}' for name in self.__preproc_instr_callbacks))
      raise self.context.MakeLocationError(
          self.__Location(),
          f"unknown pre-processing instruction: '$${preproc_instr_name}'\n"
          f"known instructions: {known}")
    preproc_instr_callback()
    self.__UpdateLineno(value)
    self.__skip_spaces = True

  def __PreprocessWhitespacePreserve(self):
    self.__text_processor = self.__TextProcessorPreserveWhitespace

  def __PreprocessWhitespaceSkip(self):
    self.__text_processor = self.__TextProcessorSkipWhitespace

  # Macro

  @rule(r'\$' + MACRO_NAME_PATTERN)
  def RuleMacro(self, value):
    macro_name = value[1:]
    if VALID_MACRO_NAME_REGEXP.match(macro_name) is None:
      return self.RuleMacroInvalid(value)
    token = self.__MacroToken(value[1:])
    self.__skip_spaces |= self.__text_processor.skip_whitespace_after_macro
    return token

  @rule(r'\$(?:[^$]\S{,9}|\Z)')
  def RuleMacroInvalid(self, value):
    raise self.context.MakeLocationError(self.__Location(),
                                         f"invalid macro name: '{value}'")

  # Special characters

  @rule(r'%', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RulePercent(self, _):
    return self.__MacroToken('text.percent')

  @rule(r'&', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleAmpersand(self, _):
    return self.__MacroToken('text.ampersand')

  @rule(r'\\', rule_type=RuleType.SPECIAL_CHAR_LATEX)
  def RuleBackslash(self, _):
    return self.__MacroToken('text.backslash')

  @rule(r'_', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleUnderscore(self, _):
    return self.__MacroToken('text.underscore')

  @rule(r'~', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleNonBreakingSpace(self, _):
    return self.__MacroToken('text.nbsp')

  @rule(r'-{2,}', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleDashes(self, value):
    length = len(value)
    if length == 2:
      dash_name = 'en'
    elif length == 3:
      dash_name = 'em'
    else:
      return Token(TokenType.TEXT, self.__lineno, value)
    return self.__MacroToken('text.dash.' + dash_name)

  @rule(r'\.{3,}', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleEllipsis(self, value):
    if len(value) == 3:
      return self.__MacroToken('text.ellipsis')
    else:
      return Token(TokenType.TEXT, self.__lineno, value)

  @rule(r'«|\<{2,}', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleGuillemetOpen(self, value):
    if len(value) <= 2:
      return self.__MacroToken('text.guillemet.open')
    else:
      return Token(TokenType.TEXT, self.__lineno, value)

  @rule(r'»|\>{2,}', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleGuillemetClose(self, value):
    if len(value) <= 2:
      return self.__MacroToken('text.guillemet.close')
    else:
      return Token(TokenType.TEXT, self.__lineno, value)

  @rule(r"`{1,2}", rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleBacktick(self, value):
    if len(value) == 1:
      return self.__MacroToken('text.backtick')
    else:
      return self.__MacroToken('text.quote.open')

  @rule(r"'{1,2}", rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleApostrophe(self, value):
    if len(value) == 1:
      return self.__MacroToken('text.apostrophe')
    else:
      return self.__MacroToken('text.quote.close')

  @rule(r'[!:;?]+', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleDoublePunctuation(self, value):
    return (
        self.__MacroToken('text.punctuation.double'),
        self.RuleLbracket('['),
        Token(TokenType.TEXT, self.__lineno, value),
        self.RuleRbracket(']'))

  # Helpers

  def __MacroToken(self, macro_name):
    self.__skip_spaces = False
    return Token(TokenType.MACRO, self.__lineno, macro_name)


class Parser:
  """
  Parses a stream of tokens into nodes.
  """

  def __init__(self, lexer):
    self.__context = lexer.context
    self.__tokens = PeekableIterator(lexer)

  def Parse(self):
    """
    Parses the input file into a list of nodes.

    Return: (List[node])
      The parsed nodes, None on fatal error.
    """

    # Optimization: bing instance values to the local scope.
    context = self.__context
    MakeLocation = context.MakeLocation
    tokens = self.__tokens

    def MakeLocationError(lineno, *args, **kwargs):
      raise context.MakeLocationError(MakeLocation(lineno), *args, **kwargs)

    def ParseNodes(call_nest_count):
      """
      Parses the tokens into a list of nodes.

      If the call_nest_count == 0, reads all available input nodes.
      Else, reads input nodes up to (exclusive) the TokenType.RBRACKET that
      completes the argument.

      Args:
        call_nest_count: (int) The macro call nesting level, 0 for top-level.

      Returns:
        (List[node]) The parsed nodes.
      """
      nodes = []
      while True:
        token = tokens.peek()
        if token is None:
          break

        token_type = token.type

        if token_type == TokenType.TEXT:
          # Text
          next(tokens)
          nodes.append(TextNode(MakeLocation(token.lineno), token.value))

        elif token_type == TokenType.MACRO:
          # Macro call
          next(tokens)
          macro_name = token.value
          macro_location = MakeLocation(token.lineno)

          # Parse the arguments
          args = []
          while True:
            # If '[', expect a new argument; else end the macro call.
            token = tokens.peek()
            if token is None or token.type != TokenType.LBRACKET:
              break
            next(tokens)
            arg_lineno = token.lineno

            # Parse the argument nodes.
            args.append(ParseNodes(call_nest_count + 1))

            # Expect a ']'.
            token = next(tokens)
            if token is None:
              raise MakeLocationError(
                  arg_lineno, "syntax error: macro argument should be closed")
            assert token.type == TokenType.RBRACKET

          nodes.append(CallNode(macro_location, macro_name, args))

        elif token_type == TokenType.RBRACKET:
          # ']': do not consume the token, leave it to the parent macro call.
          if call_nest_count == 0:
            raise MakeLocationError(
                token.lineno, "syntax error: no macro argument to close")
          break

        else:
          # Other: error
          raise MakeLocationError(
              token.lineno, f"syntax error: '{token.value}'")
      return nodes

    return ParseNodes(0)


class PeekableIterator:

  def __init__(self, iterable):
    self.__iterable = iter(iterable)
    self.__next_value = None
    self.__has_next_value = False

  def __next__(self):
    if self.__has_next_value:
      next_value = self.__next_value
      self.__has_next_value = (next_value is None)
      return next_value
    else:
      return next(self.__iterable, None)

  def peek(self):
    if self.__has_next_value:
      return self.__next_value
    else:
      next_value = next(self.__iterable, None)
      self.__next_value = next_value
      self.__has_next_value = True
      return next_value


def ParseFile(reader, filename, logger):
  """
  Parses a file into a list of nodes.

  Does not close the file.

  Args:
    filename: (Filename) The file to parse.
    logger: (Logger) The logger to use to report errors.

  Returns:
    (List[node]) The parsed nodes.

  Throws:
    FatalError
  """
  assert isinstance(filename, Filename)
  context = ParsingContext(filename, logger)

  # Read the file entirely.
  try:
    input_text = reader.read()
  except Exception as e:
    raise context.MakeLocationError(
        context.MakeLocation(1),
        f'unable to read the input file: {filename}\n{e}') from e

  # Parse the file contents.
  lexer = Lexer(context, input_text)
  parser = Parser(lexer)
  return parser.Parse()
