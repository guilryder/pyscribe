# -*- coding: utf-8 -*-
# Copyright 2011, Guillaume Ryder, GNU GPL v3 license

__author__ = 'Guillaume Ryder'

import inspect
import itertools
import re

from log import *
from macros import MACRO_NAME_PATTERN, VALID_MACRO_NAME_REGEXP


TOKEN_LBRACKET = 0  # value: ignored
TOKEN_RBRACKET = 1  # value: ignored
TOKEN_TEXT = 2      # value: unescaped text
TOKEN_MACRO = 3     # value: macro name without '$' prefix


class Token:
  """
  Token generated by Lexer.

  Fields:
    type: (int) The type of the token, must be one of TOKEN_* constants.
    lineno: (int) The line index of the first character of the token.
    value: (String) The contents of the token.
  """

  def __init__(self, tok_type, lineno, value):
    self.type = tok_type
    self.lineno = lineno
    self.value = value

  def __repr__(self):
    return '({0.type} l{0.lineno} {0.value!r})'.format(self)


class TextNode:
  """Plain text node; may contain line breaks."""

  def __init__(self, location, text):
    self.location = location
    self.text = text

  def __str__(self):
    return repr(self.text)

  def __repr__(self):
    return "{0.location!r}{0}".format(self)

  def __eq__(self, other):
    return isinstance(other, TextNode) and repr(self) == repr(other)


class CallNode:
  """
  Macro call node.

  Fields:
    location: (Location) The location of the call node.
    name: (string) The name of the macro called, without '$' prefix.
    args: (node list list) The arguments of the macro, each as a list of nodes.
  """

  def __init__(self, location, name, args):
    self.location = location
    self.name = name
    self.args = args

  def __str__(self):
    return '${name}{args}'.format(
        name=self.name,
        args=''.join('[%s]' % FormatNodes(param) for param in self.args))

  def __repr__(self):
    return '${name}{args}'.format(
        name=self.name,
        args=''.join('[%s]' % ReprNodes(param) for param in self.args))

  def __eq__(self, other):
    return isinstance(other, CallNode) and \
           self.location == other.location and \
           self.name == other.name and \
           self.args == other.args


def CompactTextNodes(nodes):
  """
  Merges consecutive text nodes located on the same line.

  Args:
    nodes: (Node iter) The nodes to process.

  Yields: (Node) The input nodes with consecutive text nodes merged.
    The location of a merged text node is the location of the first original
    text node.
  """
  def GroupingKey(node):
    return (type(node), node.location)
  for (node_type, location), nodes in itertools.groupby(nodes, GroupingKey):
    if issubclass(node_type, TextNode):
      nodes = list(nodes)
      yield TextNode(location, ''.join(node.text for node in nodes))
    else:
      for node in nodes:
        yield node


def FormatNodes(nodes):
  """Formats the given text nodes into a human-readable string."""
  return ''.join(str(node) for node in CompactTextNodes(nodes))


def ReprNodes(nodes):
  """Formats the given text nodes into a representation string."""
  return ''.join(repr(node) for node in nodes)


class ParsingContext:
  """
  Context for parsing an input file.

  Fields:
    filename: (Filename) The name of the file parsed.
    logger: (Logger) The logger to use to report errors
  """

  def __init__(self, filename, logger):
    assert isinstance(filename, Filename)
    self.filename = filename
    self.logger = logger

  def Location(self, lineno):
    """Builds a Location object for this context."""
    return Location(self.filename, lineno)

  def FatalError(self, *args, **kwargs):
    """
    Raises a fatal error.

    See Logger.Log() for parameters.
    """
    raise self.logger.Log(*args, **kwargs)


class RegexpParser:
  """
  Regexp-based parser.

  Creates a parsing rule for each method starting with 'Rule' in an object.
  The docstring of the method is expected to be a regexp.
  """

  def __init__(self, rules_container):
    """
    Args:
      rules_container: (object) The object that contains the rules.
        Each rule must be prefixed with 'Rule' and have a regexp as docstring.
    """

    # Retrieve the rules from the container.
    self.__rules = {}
    rules = []  # (name, pattern) list
    rule_pattern = re.compile('Rule(?P<name>.+)')
    for symbol_name, symbol in inspect.getmembers(rules_container):
      rule_match = rule_pattern.match(symbol_name)
      if rule_match:
        rule_name = rule_match.group('name')
        rule_regexp = symbol.__doc__
        assert rule_regexp is not None, 'Regexp missing in ' + symbol_name
        rules.append((rule_name, rule_regexp))
        self.__rules[rule_name] = symbol

    # Compute the aggregate regexp for all rules.
    full_pattern = '|'.join(
        '(?P<{name}>{pattern})'.format(name=name, pattern=pattern)
        for name, pattern in rules)
    self.__regexp = re.compile(full_pattern, re.MULTILINE)

  def Parse(self, input_text):
    """
    Parses the given input text.

    Args:
      input_text: (string) The text to parse.

    Yields:
      (string, callable|None, string|None) The non-overlapping matches of the
      rules, each as a (text_before, rule_callable, matched_text) tuple.
      'text_before' is the text between the previous match (or the beginning of
      the string for the first match) and the current match.
      'rule_callable' and 'matched_text' are the rule and the text that matched,
      None after the last match if there is text after it.
    """
    rules = self.__rules
    last_end = 0

    for match in self.__regexp.finditer(input_text):
      (match_begin, match_end) = match.span()
      rule_name = match.lastgroup
      yield (input_text[last_end:match_begin],
             rules[rule_name], match.group(rule_name))
      last_end = match_end

    if last_end < len(input_text):
      yield (input_text[last_end:], None, None)


class Lexer:

  __GLOBAL_STRIP_REGEXP = re.compile(r'(?:|\^|\s*(.*?)(?<=[^\^])\s*\^?)\Z',
                              re.MULTILINE | re.DOTALL)
  __LINE_REGEXP = re.compile(r'[ \t]*(\n+)[ \t]*')

  def __init__(self, context, input_text):
    # Strip whitespace around the input text.
    input_text_match = self.__GLOBAL_STRIP_REGEXP.match(input_text)
    self.__lineno = 1 + input_text[:input_text_match.start(1)].count('\n')
    self.__skip_spaces = True
    self.__input_text = input_text_match.group(1) or ''

    self.context = context
    self.__filename = context.filename
    self.__parser = RegexpParser(self)
    self.__tokens = self.__MergeTextTokensSameLine(self.__Parse())
    self.__preproc_instr_callbacks = {
        'whitespace.preserve': self.__PreprocessWhitespacePreserve,
        'whitespace.skip': self.__PreprocessWhitespaceSkip,
    }
    self.__text_processor = self.__TextProcessorPreserveWhitespace

  def __iter__(self):
    """Returns the tokens iterator."""
    return self.__tokens

  @staticmethod
  def __MergeTextTokensSameLine(tokens):
    """
    Merges the consecutive text tokens that have the same line.

    Args:
      nodes: (Token iter) The tokens to process.

    Yields: (Token)
    """
    text_token_accu = None
    for token in tokens:
      if token.type == TOKEN_TEXT:
        # Text token
        if text_token_accu:
          if text_token_accu.lineno == token.lineno:
            # Same line: append the text token to the accumulator.
            text_token_accu.value += token.value
            continue
          yield text_token_accu
          text_token_accu = None
          yield token
        else:
          # Start a new text accumulator.
          text_token_accu = token
      else:
        # Not a text token: flush the text accumulator (if any) then the token.
        if text_token_accu:
          yield text_token_accu
          text_token_accu = None
        yield token

    if text_token_accu:
      yield text_token_accu

  def __Parse(self):
    """
    Parses the tokens.

    Yields: (Token) The parsed tokens.
    """
    from collections import Iterable
    for text_before, rule_callable, matched_text in \
        self.__parser.Parse(self.__input_text):
      if text_before:
        if self.__skip_spaces:
          text_before = text_before.lstrip(' \t')
        for token in self.__text_processor(text_before):
          yield token
        self.__skip_spaces = (text_before and text_before[-1] == '\n')
      if rule_callable:
        token = rule_callable(matched_text)
        if isinstance(token, Iterable):
          for single_token in token:
            yield single_token
        elif token:
          yield token

  def __TextProcessorPreserveWhitespace(self, text):
    """
    Yields tokens for a block of text, preserving whitespace.

    Ensures that line returns are always at the end of a token.
    Strips spaces around each line.
    """
    lineno = self.__lineno

    # Yield the text before each sequence of newlines,
    # including the newlines themselves but without surrounding spaces.
    last_end = 0
    for match in self.__LINE_REGEXP.finditer(text):
      (match_begin, match_end) = match.span()
      newlines = match.group(1)
      yield Token(TOKEN_TEXT, lineno, text[last_end:match_begin] + newlines)
      lineno += len(newlines)
      last_end = match_end

    # Yield the last chunk of text.
    if last_end < len(text):
      yield Token(TOKEN_TEXT, lineno, text[last_end:])

    self.__lineno = lineno
    self.__skip_spaces = False
  __TextProcessorPreserveWhitespace.skip_whitespace_after_macro = False

  def __TextProcessorSkipWhitespace(self, text):
    """
    Sames as __TextProcessorPreserveWhitespace, but skips more whitespace.

    Skips newlines and whitespace after macros.
    """
    lineno = self.__lineno

    # Yield the text before each sequence of newlines.
    last_end = 0
    for match in self.__LINE_REGEXP.finditer(text):
      (match_begin, match_end) = match.span()
      if last_end < match_begin:
        yield Token(TOKEN_TEXT, lineno, text[last_end:match_begin])
      lineno += len(match.group(1))
      last_end = match_end

    # Yield the last chunk of text.
    if last_end < len(text):
      yield Token(TOKEN_TEXT, lineno, text[last_end:])

    self.__lineno = lineno
    self.__skip_spaces = False
  __TextProcessorSkipWhitespace.skip_whitespace_after_macro = True

  def __Location(self):
    return Location(self.__filename, self.__lineno)

  def __UpdateLineno(self, text):
    if text:
      self.__lineno += text.count('\n')
      self.__skip_spaces = (text[-1] == '\n')

  def RuleComment(self, value):
    r'[ \t]*\#.*(?:\n\s*|\Z)'
    self.__UpdateLineno(value)
    return None

  def RuleEscape(self, value):
    r'\^.'
    self.__skip_spaces = False
    return Token(TOKEN_TEXT, self.__lineno, value[1:])

  def RuleLbracket(self, value):
    r'\[\s*'
    token = Token(TOKEN_LBRACKET, self.__lineno, '[')
    self.__UpdateLineno(value)
    return token

  def RuleRbracket(self, value):
    r'\s*\]'
    self.__UpdateLineno(value)
    return Token(TOKEN_RBRACKET, self.__lineno, value)

  # Pre-processing statement

  def RulePreProcessingInstruction(self, value):
    r'\$\$[a-zA-Z0-9._]*\n?'
    preproc_instr_name = value[2:].strip()
    preproc_instr_callback = \
        self.__preproc_instr_callbacks.get(preproc_instr_name)
    if preproc_instr_callback:
      preproc_instr_callback()
    else:
      self.context.FatalError(
          self.__Location(),
          "unknown pre-processing instruction: '{name}'\n" +
          "known instructions: {known}",
          name='$$' + preproc_instr_name,
          known=', '.join(sorted((
              '$$' + name for name in self.__preproc_instr_callbacks))))
    self.__UpdateLineno(value)
    self.__skip_spaces = True
    return None

  def __PreprocessWhitespacePreserve(self):
    self.__text_processor = self.__TextProcessorPreserveWhitespace

  def __PreprocessWhitespaceSkip(self):
    self.__text_processor = self.__TextProcessorSkipWhitespace

  # Macro

  def RuleMacro(self, value):
    macro_name = value[1:]
    if not VALID_MACRO_NAME_REGEXP.match(macro_name):
      return self.RuleMacroInvalid(value)
    token = self.__MacroToken(value[1:])
    self.__skip_spaces |= self.__text_processor.skip_whitespace_after_macro
    return token
  RuleMacro.__doc__ = r'\$' + MACRO_NAME_PATTERN

  def RuleMacroInvalid(self, value):
    r'\$(?:[^$]\S{,9}|\Z)'
    self.context.FatalError(self.__Location(),
                            "invalid macro name: '{name}'",
                            name=value)

  # Special characters

  def RulePercent(self, _):
    r'%'
    return self.__MacroToken('text.percent')

  def RuleAmpersand(self, _):
    r'&'
    return self.__MacroToken('text.ampersand')

  def RuleUnderscore(self, _):
    r'_'
    return self.__MacroToken('text.underscore')

  def RuleNonBreakingSpace(self, _):
    r'~'
    return self.__MacroToken('text.nbsp')

  def RuleDashes(self, value):
    r'-{2,}'
    length = len(value)
    if length == 2:
      dash_name = 'en'
    elif length == 3:
      dash_name = 'em'
    else:
      return Token(TOKEN_TEXT, self.__lineno, value)
    return self.__MacroToken('text.dash.' + dash_name)

  def RuleEllipsis(self, value):
    r'\.{3,}'
    if len(value) == 3:
      return self.__MacroToken('text.ellipsis')
    else:
      return Token(TOKEN_TEXT, self.__lineno, value)

  def RuleGuillemetOpen(self, value):
    r'«|\<{2,}'
    if len(value) <= 2:
      return self.__MacroToken('text.guillemet.open')
    else:
      return Token(TOKEN_TEXT, self.__lineno, value)

  def RuleGuillemetClose(self, value):
    r'»|\>{2,}'
    if len(value) <= 2:
      return self.__MacroToken('text.guillemet.close')
    else:
      return Token(TOKEN_TEXT, self.__lineno, value)

  def RuleBacktick(self, unused_value):
    r"`"
    return self.__MacroToken('text.backtick')

  def RuleApostrophe(self, unused_value):
    r"'"
    return self.__MacroToken('text.apostrophe')

  def RuleDoublePunctuation(self, value):
    r'[!:;?]+'
    return (
        self.__MacroToken('text.punctuation.double'),
        self.RuleLbracket('['),
        Token(TOKEN_TEXT, self.__lineno, value),
        self.RuleRbracket(']'))

  # Helpers

  def __MacroToken(self, macro_name):
    self.__skip_spaces = False
    return Token(TOKEN_MACRO, self.__lineno, macro_name)


class Parser:
  """
  Parses a stream of tokens into nodes.
  """

  def __init__(self, lexer):
    self.__context = lexer.context
    self.__tokens = PeekableIterator(lexer)

  def Parse(self):
    """
    Parses the input file into a list of nodes.

    Return: (node list)
      The parsed nodes, None on fatal error.
    """

    # Optimization: bing instance values to the local scope.
    context = self.__context
    Location = context.Location
    Log = context.logger.Log
    tokens = self.__tokens

    def ParseNodes(call_nest_count):
      """
      Parses the tokens into a list of nodes.

      If the call_nest_count == 0, reads all available input nodes.
      Else, reads input nodes up to (exclusive) the TOKEN_RBRACKET that
      completes the argument.

      Args:
        call_nest_count: (int) The macro call nesting level, 0 for top-level.

      Returns:
        (node list) The parsed nodes.
      """
      nodes = []
      while True:
        token = tokens.peek()
        if not token:
          break

        token_type = token.type

        if token_type == TOKEN_TEXT:
          # Text
          next(tokens)
          nodes.append(TextNode(Location(token.lineno), token.value))

        elif token_type == TOKEN_MACRO:
          # Macro call
          next(tokens)
          macro_name = token.value
          macro_location = Location(token.lineno)

          # Parse the arguments
          args = []
          while True:
            # If '[', expect a new argument; else end the macro call.
            token = tokens.peek()
            if not token or token.type != TOKEN_LBRACKET:
              break
            next(tokens)
            arg_lineno = token.lineno

            # Parse the argument nodes.
            args.append(ParseNodes(call_nest_count + 1))

            # Expect a ']'.
            token = next(tokens)
            if not token:
              raise Log(Location(arg_lineno),
                        "syntax error: macro argument should be closed")
            assert token.type == TOKEN_RBRACKET

          nodes.append(CallNode(macro_location, macro_name, args))

        elif token_type == TOKEN_RBRACKET:
          # ']': do not consume the token, leave it to the parent macro call.
          if call_nest_count == 0:
            raise Log(Location(token.lineno),
                      "syntax error: no macro argument to close")
          break

        else:
          # Other: error
          raise Log(Location(token.lineno), "syntax error: '{token.value}'",
                    token=token)
      return nodes

    return ParseNodes(0)


class PeekableIterator:

  def __init__(self, iterable):
    self.__iterable = iter(iterable)
    self.__next_value = None
    self.__has_next_value = False

  def __next__(self):
    if self.__has_next_value:
      next_value = self.__next_value
      self.__has_next_value = (next_value is None)
      return next_value
    else:
      return next(self.__iterable, None)

  def peek(self):
    if self.__has_next_value:
      return self.__next_value
    else:
      next_value = next(self.__iterable, None)
      self.__next_value = next_value
      self.__has_next_value = True
      return next_value


def ParseFile(reader, filename, logger):
  """
  Parses a file into a list of nodes.

  Args:
    filename: (Filename) The file to parse.
    logger: (Logger) The logger to use to report errors.

  Returns:
    (node list) The parsed nodes.

  Throws:
    FatalError
  """
  assert isinstance(filename, Filename)
  context = ParsingContext(filename, logger)

  # Read the file entirely.
  try:
    input_text = reader.read()
    reader.close()
  except Exception as e:
    context.FatalError(
        context.Location(1),
        'unable to read the input file: {filename}\n{error}'.format(
            filename=filename, error=e))

  # Parse the file contents.
  lexer = Lexer(context, input_text)
  parser = Parser(lexer)
  return parser.Parse()
