# -*- coding: utf-8 -*-
# Copyright 2011, Guillaume Ryder, GNU GPL v3 license

from __future__ import annotations

__author__ = 'Guillaume Ryder'

import collections
from collections.abc import Iterable, Iterator, Sequence
from dataclasses import dataclass
import enum
import inspect
import itertools
import re
from typing import Any, NoReturn, Optional, TextIO, TYPE_CHECKING, TypeVar

from log import Filename, Location
from macros import MACRO_NAME_PATTERN, VALID_MACRO_NAME_REGEXP

if TYPE_CHECKING:
  from execution import Executor
  from log import FatalError, Logger


class TokenType(enum.Enum):
  LBRACKET = enum.auto()  # value: ignored
  RBRACKET = enum.auto()  # value: ignored
  TEXT = enum.auto()      # value: unescaped text
  MACRO = enum.auto()     # value: macro name without '$' prefix


@dataclass
class Token:
  """Token generated by Lexer."""

  type: TokenType
  lineno: int  # The 1-based line index of the first character of the token.
  value: str  # The contents of the token; mutable.

  def __repr__(self) -> str:
    return f'({self.type} l{self.lineno} {self.value!r})'


class Node:
  """Base class for nodes."""

  location: Location


NodesT = Sequence[Node]


@dataclass(frozen=True)
class TextNode(Node):
  """Plain text node; may contain line breaks."""

  location: Location
  text: str

  def Execute(self, executor: 'Executor') -> None:
    executor.AppendText(self.text)

  def __str__(self) -> str:
    return repr(self.text)

  def __repr__(self) -> str:
    return f'{self.location!r}{self}'


@dataclass(frozen=True)
class CallNode(Node):
  """Macro call node."""

  location: Location
  name: str  # The name of the macro called, without '$' prefix.
  args: Sequence[NodesT]  # The macro arguments.

  def Execute(self, executor: 'Executor') -> None:
    executor.CallMacro(self)

  def __str__(self) -> str:
    args = ''.join(f'[{FormatNodes(param)}]' for param in self.args)
    return f'${self.name}{args}'

  def __repr__(self) -> str:
    args = ''.join(f'[{ReprNodes(param)}]' for param in self.args)
    return f'${self.name}{args}'


def CompactTextNodes(nodes) -> Iterator[Node]:
  """Merges consecutive text nodes located on the same line.

  Args:
    nodes: The nodes to process.

  Yields:
    The input nodes with consecutive text nodes merged. The location of a
    merged text node is the location of the first original text node.
  """
  def GroupingKey(node) -> tuple[type[Node], Location]:
    return type(node), node.location
  for (node_type, location), nodes_grp in itertools.groupby(nodes, GroupingKey):
    if issubclass(node_type, TextNode):
      yield TextNode(location, ''.join(node.text for node in nodes_grp))
    else:
      yield from nodes_grp


def FormatNodes(nodes: Iterable[Node]) -> str:
  """Formats the given text nodes into a human-readable string."""
  return ''.join(str(node) for node in CompactTextNodes(nodes))


def ReprNodes(nodes: Iterable[Node]) -> str:
  """Formats the given text nodes into a representation string."""
  return ''.join(repr(node) for node in nodes)


@dataclass(frozen=True)
class ParsingContext:
  """Context for parsing an input file."""

  filename: Filename  # The name of the file parsed.
  logger: 'Logger'

  def MakeLocation(self, lineno: int) -> Location:
    """Builds a Location object for this context."""
    return Location(self.filename, lineno)

  def MakeLocationError(self, *args: Any, **kwargs: Any) -> 'FatalError':
    """Returns a fatal error for the current location.

    See Logger.LocationError() for parameters.
    """
    return self.logger.LocationError(*args, **kwargs)


_RULE_METHOD_NAME_PATTERN = re.compile('Rule(?P<name>.+)')

class RuleType(enum.Flag):
  REGULAR = enum.auto()
  SPECIAL_CHAR_LATEX = enum.auto()
  SPECIAL_CHAR_OTHER = enum.auto()
  ALL = REGULAR | SPECIAL_CHAR_LATEX | SPECIAL_CHAR_OTHER

def rule(regexp, rule_type=RuleType.REGULAR):
  """Decorator for rule methods; see RegexpParser."""
  def wrapper(func):
    name_match = _RULE_METHOD_NAME_PATTERN.match(func.__name__)
    assert name_match, f'Invalid @rule function name: {func.__name__}'
    func.rule_name = name_match.group('name')
    func.regexp = regexp
    func.rule_type = rule_type
    return func
  return wrapper


class RegexpParser:
  """
  Regexp-based parser.

  Creates a parsing rule for each method of an object decorated with @rule.

  Fields:
    __rules: (OrderedDict[name, method])
  """

  def __init__(self, rules_container: object):
    """
    Args:
      rules_container: The object that contains the @rule methods.
    """

    # Retrieve the rules from the container, sorted alphabetically.
    self.__rules = collections.OrderedDict((
        (rule.rule_name, rule)
        for _, rule in inspect.getmembers(rules_container,
                                          lambda mem: hasattr(mem, 'rule_name'))
    ))

    # Compute the aggregate regexp for all rules.
    full_pattern = '|'.join(
        f'(?P<{rule.rule_name}>{rule.regexp})'
        for rule in self.__rules.values())
    self.__regexp = re.compile(full_pattern, re.MULTILINE)

  def Parse(self, input_text):
    """Parses the given input text.

    Args:
      input_text: The text to parse.

    Yields:
      (Tuple[str, callable|None, str|None]) The non-overlapping matches of the
      rules, each as a Tuple[text_before, rule_callable, matched_text].
      'text_before' is the text between the previous match (or the beginning of
      the string for the first match) and the current match.
      'rule_callable' and 'matched_text' are the rule and the text that matched,
      None after the last match if there is text after it.
    """
    rules = self.__rules
    last_end = 0

    for match in self.__regexp.finditer(input_text):
      match_begin, match_end = match.span()
      rule_name = match.lastgroup
      yield (input_text[last_end:match_begin],
             rules[rule_name], match.group(rule_name))
      last_end = match_end

    if last_end < len(input_text):
      yield (input_text[last_end:], None, None)


class Lexer:

  __GLOBAL_STRIP_REGEXP = re.compile(r'(?:|\^|\s*(.*?)(?<=[^\^])\s*\^?)\Z',
                              re.MULTILINE | re.DOTALL)
  __LINE_REGEXP = re.compile(r'[ \t]*(\n+)[ \t]*')

  def __init__(self, context, input_text):
    # Strip whitespace around the input text.
    input_text_match = self.__GLOBAL_STRIP_REGEXP.match(input_text)
    assert input_text_match
    self.__lineno = 1 + input_text[:input_text_match.start(1)].count('\n')
    self.__skip_spaces = True
    self.__enabled_rule_types = RuleType.ALL
    self.__input_text = input_text_match.group(1) or ''

    self.context = context
    self.__filename = context.filename
    self.__parser = RegexpParser(self)
    self.__tokens = self.__MergeTextTokensSameLine(self.__Parse())

    def SetEnabledRuleTypes(bitset):
      self.__enabled_rule_types = bitset

    self.__preproc_instr_callbacks = {
        'whitespace.preserve': self.__PreprocessWhitespacePreserve,
        'whitespace.skip': self.__PreprocessWhitespaceSkip,
        'special.chars.escape.all': lambda: SetEnabledRuleTypes(
            RuleType.ALL),
        'special.chars.escape.none': lambda: SetEnabledRuleTypes(
            RuleType.REGULAR),
        'special.chars.latex.mode': lambda: SetEnabledRuleTypes(
            RuleType.REGULAR | RuleType.SPECIAL_CHAR_OTHER),
    }
    self.__text_processor = self.__TextProcessorPreserveWhitespace

  def __iter__(self) -> Iterator[Token]:
    """Returns the tokens iterator."""
    return self.__tokens

  @staticmethod
  def __MergeTextTokensSameLine(tokens: Iterable[Token]) -> Iterator[Token]:
    """Merges the consecutive text tokens that have the same line."""
    text_token_accu = None
    for token in tokens:
      if token.type == TokenType.TEXT:
        # Text token
        if text_token_accu is None:
          # Start a new text accumulator.
          text_token_accu = token
        else:
          if text_token_accu.lineno == token.lineno:
            # Same line: append the text token to the accumulator.
            text_token_accu.value += token.value
            continue
          yield text_token_accu
          text_token_accu = None
          yield token
      else:
        # Not a text token: flush the text accumulator (if any) then the token.
        if text_token_accu is not None:
          yield text_token_accu
          text_token_accu = None
        yield token

    if text_token_accu is not None:
      yield text_token_accu

  def __Parse(self) -> Iterator[Token]:
    """Parses the tokens."""
    # pylint: disable=import-outside-toplevel,redefined-outer-name,reimported
    from collections.abc import Iterable
    for text_before, rule_callable, matched_text in (
        self.__parser.Parse(self.__input_text)):
      if text_before is not None:
        if self.__skip_spaces:
          text_before = text_before.lstrip(' \t')
        yield from self.__text_processor(text_before)
        self.__skip_spaces = (bool(text_before) and text_before[-1] == '\n')
      if rule_callable is not None:
        if self.__enabled_rule_types & rule_callable.rule_type:
          token = rule_callable(matched_text)
          if isinstance(token, Iterable):
            yield from token
          elif token is not None:
            yield token
        else:
          yield Token(TokenType.TEXT, self.__lineno, matched_text)

  def __TextProcessorPreserveWhitespace(self, text: str) -> Iterator[Token]:
    """Yields tokens for a block of text, preserving whitespace.

    Ensures that line returns are always at the end of a token.
    Strips spaces around each line.
    """
    lineno = self.__lineno

    # Yield the text before each sequence of newlines,
    # including the newlines themselves but without surrounding spaces.
    last_end = 0
    for match in self.__LINE_REGEXP.finditer(text):
      match_begin, match_end = match.span()
      newlines = match.group(1)
      yield Token(TokenType.TEXT, lineno, text[last_end:match_begin] + newlines)
      lineno += len(newlines)
      last_end = match_end

    # Yield the last chunk of text.
    if last_end < len(text):
      yield Token(TokenType.TEXT, lineno, text[last_end:])

    self.__lineno = lineno
    self.__skip_spaces = False

  def __TextProcessorSkipWhitespace(self, text: str) -> Iterator[Token]:
    """Sames as __TextProcessorPreserveWhitespace, but skips more whitespace.

    Skips newlines and whitespace after macros.
    """
    lineno = self.__lineno

    # Yield the text before each sequence of newlines.
    last_end = 0
    for match in self.__LINE_REGEXP.finditer(text):
      match_begin, match_end = match.span()
      if last_end < match_begin:
        yield Token(TokenType.TEXT, lineno, text[last_end:match_begin])
      lineno += len(match.group(1))
      last_end = match_end

    # Yield the last chunk of text.
    if last_end < len(text):
      yield Token(TokenType.TEXT, lineno, text[last_end:])

    self.__lineno = lineno
    self.__skip_spaces = False

  def __Location(self) -> Location:
    return Location(self.__filename, self.__lineno)

  def __UpdateLineno(self, text: Optional[str]) -> None:
    if text:
      self.__lineno += text.count('\n')
      self.__skip_spaces = (text[-1] == '\n')

  @rule(r'[ \t]*\#.*(?:\n\s*|\Z)')
  def RuleComment(self, value: str) -> None:
    self.__UpdateLineno(value)

  @rule(r'\^.')
  def RuleEscape(self, value: str) -> Token:
    self.__skip_spaces = False
    return Token(TokenType.TEXT, self.__lineno, value[1:])

  @rule(r'\[\s*')
  def RuleLbracket(self, value: str) -> Token:
    token = Token(TokenType.LBRACKET, self.__lineno, '[')
    self.__UpdateLineno(value)
    return token

  @rule(r'\s*\]')
  def RuleRbracket(self, value: str) -> Token:
    self.__UpdateLineno(value)
    return Token(TokenType.RBRACKET, self.__lineno, value)

  # Pre-processing statement

  @rule(r'\$\$[a-zA-Z0-9._]*\n?')
  def RulePreProcessingInstruction(self, value: str) -> None:
    preproc_instr_name = value[2:].strip()
    preproc_instr_callback = (
        self.__preproc_instr_callbacks.get(preproc_instr_name))
    if preproc_instr_callback is None:
      known = ', '.join(sorted(
          f'$${name}' for name in self.__preproc_instr_callbacks))
      raise self.context.MakeLocationError(
          self.__Location(),
          f"unknown pre-processing instruction: '$${preproc_instr_name}'\n"
          f"known instructions: {known}")
    preproc_instr_callback()
    self.__UpdateLineno(value)
    self.__skip_spaces = True

  def __PreprocessWhitespacePreserve(self) -> None:
    self.__text_processor = self.__TextProcessorPreserveWhitespace

  def __PreprocessWhitespaceSkip(self) -> None:
    self.__text_processor = self.__TextProcessorSkipWhitespace

  # Macro

  @rule(r'\$' + MACRO_NAME_PATTERN)
  def RuleMacro(self, value: str) -> Token:
    macro_name = value[1:]
    if VALID_MACRO_NAME_REGEXP.match(macro_name) is None:
      return self.RuleMacroInvalid(value)
    token = self.__MacroToken(value[1:])
    self.__skip_spaces |= (
        self.__text_processor == self.__TextProcessorSkipWhitespace)
    return token

  @rule(r'\$(?:[^$]\S{,9}|\Z)')
  def RuleMacroInvalid(self, value: str) -> NoReturn:
    raise self.context.MakeLocationError(self.__Location(),
                                         f"invalid macro name: '{value}'")

  # Special characters

  @rule(r'%', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RulePercent(self, _: str) -> Token:
    return self.__MacroToken('text.percent')

  @rule(r'&', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleAmpersand(self, _: str) -> Token:
    return self.__MacroToken('text.ampersand')

  @rule(r'\\', rule_type=RuleType.SPECIAL_CHAR_LATEX)
  def RuleBackslash(self, _: str) -> Token:
    return self.__MacroToken('text.backslash')

  @rule(r'_', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleUnderscore(self, _: str) -> Token:
    return self.__MacroToken('text.underscore')

  @rule(r'~', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleNonBreakingSpace(self, _: str) -> Token:
    return self.__MacroToken('text.nbsp')

  @rule(r'-{2,}', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleDashes(self, value: str) -> Token:
    length = len(value)
    if length == 2:
      dash_name = 'en'
    elif length == 3:
      dash_name = 'em'
    else:
      return Token(TokenType.TEXT, self.__lineno, value)
    return self.__MacroToken('text.dash.' + dash_name)

  @rule(r'\.{3,}', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleEllipsis(self, value: str) -> Token:
    if len(value) == 3:
      return self.__MacroToken('text.ellipsis')
    else:
      return Token(TokenType.TEXT, self.__lineno, value)

  @rule(r'«|\<{2,}', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleGuillemetOpen(self, value: str) -> Token:
    if len(value) <= 2:
      return self.__MacroToken('text.guillemet.open')
    else:
      return Token(TokenType.TEXT, self.__lineno, value)

  @rule(r'»|\>{2,}', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleGuillemetClose(self, value: str) -> Token:
    if len(value) <= 2:
      return self.__MacroToken('text.guillemet.close')
    else:
      return Token(TokenType.TEXT, self.__lineno, value)

  @rule(r"`{1,2}", rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleBacktick(self, value: str) -> Token:
    if len(value) == 1:
      return self.__MacroToken('text.backtick')
    else:
      return self.__MacroToken('text.quote.open')

  @rule(r"'{1,2}", rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleApostrophe(self, value: str) -> Token:
    if len(value) == 1:
      return self.__MacroToken('text.apostrophe')
    else:
      return self.__MacroToken('text.quote.close')

  @rule(r'[!:;?]+', rule_type=RuleType.SPECIAL_CHAR_OTHER)
  def RuleDoublePunctuation(self, value: str) -> Sequence[Token]:
    return (
        self.__MacroToken('text.punctuation.double'),
        self.RuleLbracket('['),
        Token(TokenType.TEXT, self.__lineno, value),
        self.RuleRbracket(']'))

  # Helpers

  def __MacroToken(self, macro_name: str) -> Token:
    self.__skip_spaces = False
    return Token(TokenType.MACRO, self.__lineno, macro_name)


class Parser:
  """Parses a stream of tokens into nodes."""

  __context: ParsingContext
  __tokens: PeekableIterator[Token]

  def __init__(self, lexer: Lexer):
    self.__context = lexer.context
    self.__tokens = PeekableIterator(lexer)

  def Parse(self) -> list[Node]:
    """Parses the input file into a list of nodes.

    Return:
      The parsed nodes.

    Raises:
      FatalError
    """

    # Optimization: bing instance values to the local scope.
    context = self.__context
    MakeLocation = context.MakeLocation
    tokens = self.__tokens

    def MakeLocationError(
        lineno: int, *args: Any, **kwargs: Any) -> 'FatalError':
      raise context.MakeLocationError(MakeLocation(lineno), *args, **kwargs)

    def ParseNodes(call_nest_count: int) -> list[Node]:
      """
      Parses the tokens into a list of nodes.

      If the call_nest_count == 0, reads all available input nodes.
      Else, reads input nodes up to (exclusive) the TokenType.RBRACKET that
      completes the argument.

      Args:
        call_nest_count: The macro call nesting level, 0 for top-level.

      Returns:
        The parsed nodes.
      """
      nodes: list[Node] = []
      while True:
        token = tokens.peek()
        if token is None:
          break

        token_type = token.type

        if token_type == TokenType.TEXT:
          # Text
          next(tokens)
          nodes.append(TextNode(MakeLocation(token.lineno), token.value))

        elif token_type == TokenType.MACRO:
          # Macro call
          next(tokens)
          macro_name = token.value
          macro_location = MakeLocation(token.lineno)

          # Parse the arguments
          args = []
          while True:
            # If '[', expect a new argument; else end the macro call.
            token = tokens.peek()
            if token is None or token.type != TokenType.LBRACKET:
              break
            next(tokens)
            arg_lineno = token.lineno

            # Parse the argument nodes.
            args.append(ParseNodes(call_nest_count + 1))

            # Expect a ']'.
            token = next(tokens, None)
            if token is None:
              raise MakeLocationError(
                  arg_lineno, "syntax error: macro argument should be closed")
            assert token.type == TokenType.RBRACKET

          nodes.append(CallNode(macro_location, macro_name, args))

        elif token_type == TokenType.RBRACKET:
          # ']': do not consume the token, leave it to the parent macro call.
          if call_nest_count == 0:
            raise MakeLocationError(
                token.lineno, "syntax error: no macro argument to close")
          break

        else:
          # Other: error
          raise MakeLocationError(
              token.lineno, f"syntax error: '{token.value}'")
      return nodes

    return ParseNodes(0)


_T = TypeVar('_T')

class PeekableIterator(Iterator[_T]):
  """Adds peek() to the given iterator.

  Limitation: does not support iterated None values.
  """

  def __init__(self, iterable: Iterable[_T]):
    self.__iterator = iter(iterable)
    self.__next_value: Optional[_T] = None
    self.__has_next_value = False

  def __next__(self) -> _T:
    if self.__has_next_value:
      next_value = self.__next_value
      if next_value is None:
        raise StopIteration
      self.__has_next_value = False
      return next_value
    else:
      return next(self.__iterator)

  def peek(self) -> Optional[_T]:
    if self.__has_next_value:
      return self.__next_value
    else:
      next_value = next(self.__iterator, None)
      self.__next_value = next_value
      self.__has_next_value = True
      return next_value


def ParseFile(reader: TextIO, filename: Filename, logger: Logger) -> list[Node]:
  """Parses a file into a list of nodes.

  Does not close the file.

  Args:
    reader: The stream to parse.
    filename: The name of the read file, for logging.
    logger: The logger to use to report errors.

  Returns:
    The parsed nodes.

  Raises:
    FatalError
  """
  context = ParsingContext(filename, logger)

  # Read the file entirely.
  try:
    input_text = reader.read()
  except Exception as e:
    raise context.MakeLocationError(
        context.MakeLocation(1),
        f'unable to read the input file: {filename}\n{e}') from e

  # Parse the file contents.
  lexer = Lexer(context, input_text)
  parser = Parser(lexer)
  return parser.Parse()
